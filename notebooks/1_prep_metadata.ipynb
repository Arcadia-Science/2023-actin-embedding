{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb66e04c",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook was used to prepare the Uniprot `features_file.tsv` to use for Cluster Mode of the ProteinCartography pipeline.\n",
    "\n",
    "In order to use this notebook, you must clone the [ProteinCartography GitHub repository](https://github.com/Arcadia-Science/ProteinCartography) and activate the `cartography` conda env. However, you should be within the `2023-actin-embedding/notebooks` folder.\n",
    "\n",
    "The original list of actins was generated in our [Defining Actin](https://research.arcadiascience.com/pub/idea-defining-actin/release/4?readingCollection=9a516d32) pub and the corresponding [Actin Prediction](https://github.com/Arcadia-Science/2022-actin-prediction) pipeline, where we did a protein BLAST search for 50,000 matches to [human beta-actin](https://www.uniprot.org/uniprotkb/P60709/entry). The list can be found in this [Zenodo archive](https://zenodo.org/records/7384393).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627755d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2e5aca-1954-4264-b2d2-05c2d0fdd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "PC_path = \"./../../ProteinCartography/ProteinCartography\"\n",
    "input_path = \"./../input\"\n",
    "sys.path.append(PC_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74773f27",
   "metadata": {},
   "source": [
    "Prepare directories. Once these are prepared, place the 2022-actin-prediction-blasoutputs.txt files in the `~/Actin/prep` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7c84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./../output/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6e7d3",
   "metadata": {},
   "source": [
    "## Map Refseq IDs\n",
    "\n",
    "The output of BLAST is a list of Refseq IDs, but ProteinCartography and the AlphaFold database reference proteins based on their Accessions/UniProt IDs. This step converts Refseq IDs to Accessions/UniProt IDs and then uses them to download the required metadata file. Note that I split this into 2 batches as the single batch was too large and continually failed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527671f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set paths\n",
    "\n",
    "map_refseqids = os.path.join(PC_path, \"map_refseqids.py\")\n",
    "input_map_refseqids1 = os.path.join(\n",
    "    input_path, \"2022-actin-prediction-blastoutputs1.txt\"\n",
    ")\n",
    "input_map_refseqids2 = os.path.join(\n",
    "    input_path, \"2022-actin-prediction-blastoutputs1.txt\"\n",
    ")\n",
    "output_map_refseqids1 = os.path.join(input_path, \"blastoutput_uniprot1.txt\")\n",
    "output_map_refseqids2 = os.path.join(input_path, \"blastoutput_uniprot2.txt\")\n",
    "\n",
    "# Batch 1\n",
    "os.system(\n",
    "    f\"python {map_refseqids} -i {input_map_refseqids1} -o {output_map_refseqids1}\"\n",
    ")\n",
    "\n",
    "# Batch 2\n",
    "os.system(\n",
    "    f\"python {map_refseqids} -i {input_map_refseqids2} -o {output_map_refseqids2}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e31249",
   "metadata": {},
   "source": [
    "## Download UniProt metadata\n",
    "\n",
    "After obtaining UniProt IDs, we use them to download metadata from UniProt. This data will be the bulk of the UniProt features file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae17cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting batch 1 of 56\n",
      "downloaded 300 / 16614 hits for batch 1\n",
      ">> Starting batch 2 of 56\n",
      "downloaded 300 / 16614 hits for batch 2\n",
      ">> Starting batch 3 of 56\n",
      "downloaded 300 / 16614 hits for batch 3\n",
      ">> Starting batch 4 of 56\n",
      "downloaded 300 / 16614 hits for batch 4\n",
      ">> Starting batch 5 of 56\n",
      "downloaded 300 / 16614 hits for batch 5\n",
      ">> Starting batch 6 of 56\n",
      "downloaded 300 / 16614 hits for batch 6\n",
      ">> Starting batch 7 of 56\n",
      "downloaded 300 / 16614 hits for batch 7\n",
      ">> Starting batch 8 of 56\n",
      "downloaded 300 / 16614 hits for batch 8\n",
      ">> Starting batch 9 of 56\n",
      "downloaded 300 / 16614 hits for batch 9\n",
      ">> Starting batch 10 of 56\n",
      "downloaded 300 / 16614 hits for batch 10\n",
      ">> Starting batch 11 of 56\n",
      "downloaded 300 / 16614 hits for batch 11\n",
      ">> Starting batch 12 of 56\n",
      "downloaded 300 / 16614 hits for batch 12\n",
      ">> Starting batch 13 of 56\n",
      "downloaded 300 / 16614 hits for batch 13\n",
      ">> Starting batch 14 of 56\n",
      "downloaded 300 / 16614 hits for batch 14\n",
      ">> Starting batch 15 of 56\n",
      "downloaded 300 / 16614 hits for batch 15\n",
      ">> Starting batch 16 of 56\n",
      "downloaded 300 / 16614 hits for batch 16\n",
      ">> Starting batch 17 of 56\n",
      "downloaded 300 / 16614 hits for batch 17\n",
      ">> Starting batch 18 of 56\n",
      "downloaded 300 / 16614 hits for batch 18\n",
      ">> Starting batch 19 of 56\n",
      "downloaded 300 / 16614 hits for batch 19\n",
      ">> Starting batch 20 of 56\n",
      "downloaded 300 / 16614 hits for batch 20\n",
      ">> Starting batch 21 of 56\n",
      "downloaded 300 / 16614 hits for batch 21\n",
      ">> Starting batch 22 of 56\n",
      "downloaded 300 / 16614 hits for batch 22\n",
      ">> Starting batch 23 of 56\n",
      "downloaded 300 / 16614 hits for batch 23\n",
      ">> Starting batch 24 of 56\n",
      "downloaded 300 / 16614 hits for batch 24\n",
      ">> Starting batch 25 of 56\n",
      "downloaded 300 / 16614 hits for batch 25\n",
      ">> Starting batch 26 of 56\n",
      "downloaded 300 / 16614 hits for batch 26\n",
      ">> Starting batch 27 of 56\n",
      "downloaded 300 / 16614 hits for batch 27\n",
      ">> Starting batch 28 of 56\n",
      "downloaded 300 / 16614 hits for batch 28\n",
      ">> Starting batch 29 of 56\n",
      "downloaded 300 / 16614 hits for batch 29\n",
      ">> Starting batch 30 of 56\n",
      "downloaded 300 / 16614 hits for batch 30\n",
      ">> Starting batch 31 of 56\n",
      "downloaded 300 / 16614 hits for batch 31\n",
      ">> Starting batch 32 of 56\n",
      "downloaded 300 / 16614 hits for batch 32\n",
      ">> Starting batch 33 of 56\n",
      "downloaded 300 / 16614 hits for batch 33\n",
      ">> Starting batch 34 of 56\n",
      "downloaded 300 / 16614 hits for batch 34\n",
      ">> Starting batch 35 of 56\n",
      "downloaded 300 / 16614 hits for batch 35\n",
      ">> Starting batch 36 of 56\n",
      "downloaded 300 / 16614 hits for batch 36\n",
      ">> Starting batch 37 of 56\n",
      "downloaded 300 / 16614 hits for batch 37\n",
      ">> Starting batch 38 of 56\n",
      "downloaded 300 / 16614 hits for batch 38\n",
      ">> Starting batch 39 of 56\n",
      "downloaded 300 / 16614 hits for batch 39\n",
      ">> Starting batch 40 of 56\n",
      "downloaded 300 / 16614 hits for batch 40\n",
      ">> Starting batch 41 of 56\n",
      "downloaded 300 / 16614 hits for batch 41\n",
      ">> Starting batch 42 of 56\n",
      "downloaded 300 / 16614 hits for batch 42\n",
      ">> Starting batch 43 of 56\n",
      "downloaded 300 / 16614 hits for batch 43\n",
      ">> Starting batch 44 of 56\n",
      "downloaded 300 / 16614 hits for batch 44\n",
      ">> Starting batch 45 of 56\n",
      "downloaded 300 / 16614 hits for batch 45\n",
      ">> Starting batch 46 of 56\n",
      "downloaded 300 / 16614 hits for batch 46\n",
      ">> Starting batch 47 of 56\n",
      "downloaded 300 / 16614 hits for batch 47\n",
      ">> Starting batch 48 of 56\n",
      "downloaded 300 / 16614 hits for batch 48\n",
      ">> Starting batch 49 of 56\n",
      "downloaded 300 / 16614 hits for batch 49\n",
      ">> Starting batch 50 of 56\n",
      "downloaded 300 / 16614 hits for batch 50\n",
      ">> Starting batch 51 of 56\n",
      "downloaded 300 / 16614 hits for batch 51\n",
      ">> Starting batch 52 of 56\n",
      "downloaded 300 / 16614 hits for batch 52\n",
      ">> Starting batch 53 of 56\n",
      "downloaded 300 / 16614 hits for batch 53\n",
      ">> Starting batch 54 of 56\n",
      "downloaded 300 / 16614 hits for batch 54\n",
      ">> Starting batch 55 of 56\n",
      "downloaded 300 / 16614 hits for batch 55\n",
      ">> Starting batch 56 of 56\n",
      "downloaded 114 / 16614 hits for batch 56\n",
      ">> Starting batch 1 of 56\n",
      "downloaded 300 / 16614 hits for batch 1\n",
      ">> Starting batch 2 of 56\n",
      "downloaded 300 / 16614 hits for batch 2\n",
      ">> Starting batch 3 of 56\n",
      "downloaded 300 / 16614 hits for batch 3\n",
      ">> Starting batch 4 of 56\n",
      "downloaded 300 / 16614 hits for batch 4\n",
      ">> Starting batch 5 of 56\n",
      "downloaded 300 / 16614 hits for batch 5\n",
      ">> Starting batch 6 of 56\n",
      "downloaded 300 / 16614 hits for batch 6\n",
      ">> Starting batch 7 of 56\n",
      "downloaded 300 / 16614 hits for batch 7\n",
      ">> Starting batch 8 of 56\n",
      "downloaded 300 / 16614 hits for batch 8\n",
      ">> Starting batch 9 of 56\n",
      "downloaded 300 / 16614 hits for batch 9\n",
      ">> Starting batch 10 of 56\n",
      "downloaded 300 / 16614 hits for batch 10\n",
      ">> Starting batch 11 of 56\n",
      "downloaded 300 / 16614 hits for batch 11\n",
      ">> Starting batch 12 of 56\n",
      "downloaded 300 / 16614 hits for batch 12\n",
      ">> Starting batch 13 of 56\n",
      "downloaded 300 / 16614 hits for batch 13\n",
      ">> Starting batch 14 of 56\n",
      "downloaded 300 / 16614 hits for batch 14\n",
      ">> Starting batch 15 of 56\n",
      "downloaded 300 / 16614 hits for batch 15\n",
      ">> Starting batch 16 of 56\n",
      "downloaded 300 / 16614 hits for batch 16\n",
      ">> Starting batch 17 of 56\n",
      "downloaded 300 / 16614 hits for batch 17\n",
      ">> Starting batch 18 of 56\n",
      "downloaded 300 / 16614 hits for batch 18\n",
      ">> Starting batch 19 of 56\n",
      "downloaded 300 / 16614 hits for batch 19\n",
      ">> Starting batch 20 of 56\n",
      "downloaded 300 / 16614 hits for batch 20\n",
      ">> Starting batch 21 of 56\n",
      "downloaded 300 / 16614 hits for batch 21\n",
      ">> Starting batch 22 of 56\n",
      "downloaded 300 / 16614 hits for batch 22\n",
      ">> Starting batch 23 of 56\n",
      "downloaded 300 / 16614 hits for batch 23\n",
      ">> Starting batch 24 of 56\n",
      "downloaded 300 / 16614 hits for batch 24\n",
      ">> Starting batch 25 of 56\n",
      "downloaded 300 / 16614 hits for batch 25\n",
      ">> Starting batch 26 of 56\n",
      "downloaded 300 / 16614 hits for batch 26\n",
      ">> Starting batch 27 of 56\n",
      "downloaded 300 / 16614 hits for batch 27\n",
      ">> Starting batch 28 of 56\n",
      "downloaded 300 / 16614 hits for batch 28\n",
      ">> Starting batch 29 of 56\n",
      "downloaded 300 / 16614 hits for batch 29\n",
      ">> Starting batch 30 of 56\n",
      "downloaded 300 / 16614 hits for batch 30\n",
      ">> Starting batch 31 of 56\n",
      "downloaded 300 / 16614 hits for batch 31\n",
      ">> Starting batch 32 of 56\n",
      "downloaded 300 / 16614 hits for batch 32\n",
      ">> Starting batch 33 of 56\n",
      "downloaded 300 / 16614 hits for batch 33\n",
      ">> Starting batch 34 of 56\n",
      "downloaded 300 / 16614 hits for batch 34\n",
      ">> Starting batch 35 of 56\n",
      "downloaded 300 / 16614 hits for batch 35\n",
      ">> Starting batch 36 of 56\n",
      "downloaded 300 / 16614 hits for batch 36\n",
      ">> Starting batch 37 of 56\n",
      "downloaded 300 / 16614 hits for batch 37\n",
      ">> Starting batch 38 of 56\n",
      "downloaded 300 / 16614 hits for batch 38\n",
      ">> Starting batch 39 of 56\n",
      "downloaded 300 / 16614 hits for batch 39\n",
      ">> Starting batch 40 of 56\n",
      "downloaded 300 / 16614 hits for batch 40\n",
      ">> Starting batch 41 of 56\n",
      "downloaded 300 / 16614 hits for batch 41\n",
      ">> Starting batch 42 of 56\n",
      "downloaded 300 / 16614 hits for batch 42\n",
      ">> Starting batch 43 of 56\n",
      "downloaded 300 / 16614 hits for batch 43\n",
      ">> Starting batch 44 of 56\n",
      "downloaded 300 / 16614 hits for batch 44\n",
      ">> Starting batch 45 of 56\n",
      "downloaded 300 / 16614 hits for batch 45\n",
      ">> Starting batch 46 of 56\n",
      "downloaded 300 / 16614 hits for batch 46\n",
      ">> Starting batch 47 of 56\n",
      "downloaded 300 / 16614 hits for batch 47\n",
      ">> Starting batch 48 of 56\n",
      "downloaded 300 / 16614 hits for batch 48\n",
      ">> Starting batch 49 of 56\n",
      "downloaded 300 / 16614 hits for batch 49\n",
      ">> Starting batch 50 of 56\n",
      "downloaded 300 / 16614 hits for batch 50\n",
      ">> Starting batch 51 of 56\n",
      "downloaded 300 / 16614 hits for batch 51\n",
      ">> Starting batch 52 of 56\n",
      "downloaded 300 / 16614 hits for batch 52\n",
      ">> Starting batch 53 of 56\n",
      "downloaded 300 / 16614 hits for batch 53\n",
      ">> Starting batch 54 of 56\n",
      "downloaded 300 / 16614 hits for batch 54\n",
      ">> Starting batch 55 of 56\n",
      "downloaded 300 / 16614 hits for batch 55\n",
      ">> Starting batch 56 of 56\n",
      "downloaded 114 / 16614 hits for batch 56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set paths\n",
    "\n",
    "fetch_uniprot_metadata = os.path.join(PC_path, \"fetch_uniprot_metadata.py\")\n",
    "output_uniprot_metadata1 = os.path.join(input_path, \"uniprot_features1.tsv\")\n",
    "output_uniprot_metadata2 = os.path.join(input_path, \"uniprot_features2.tsv\")\n",
    "\n",
    "# Batch 1\n",
    "os.system(\n",
    "    f\"python {fetch_uniprot_metadata} -i {output_map_refseqids1} -o {output_uniprot_metadata1}\"\n",
    ")\n",
    "\n",
    "# Batch 2\n",
    "os.system(\n",
    "    f\"python {fetch_uniprot_metadata} -i {output_map_refseqids2} -o {output_uniprot_metadata2}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9100365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge batched files\n",
    "uniprot_features1 = pd.read_csv(\"../input/uniprot_features1.tsv\", sep=\"\\t\")\n",
    "uniprot_features2 = pd.read_csv(\"../input/uniprot_features2.tsv\", sep=\"\\t\")\n",
    "uniprot_features_combined = pd.concat([uniprot_features1, uniprot_features2])\n",
    "uniprot_features_combined = uniprot_features_combined.drop_duplicates(subset=\"protid\", keep=\"first\")\n",
    "\n",
    "# Save uniprot_features file\n",
    "uniprot_features_combined.to_csv(\n",
    "    \"../input/uniprot_features_combined.tsv\", sep=\"\\t\", index=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf106b",
   "metadata": {},
   "source": [
    "## Filter UniProt hits\n",
    "\n",
    "We filtered UniProt hits based on fragment status and whether or not the UniProt entry was active using the `filter_uniprot_hits.py` script from [ProteinCartography](https://github.com/Arcadia-Science/ProteinCartography).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dfade35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set paths\n",
    "filter_uniprot_hits = os.path.join(PC_path, \"filter_uniprot_hits.py\")\n",
    "output_filtered_list = os.path.join(input_path, \"features_filter.tsv\")\n",
    "input_uniprot_features = os.path.join(input_path, \"uniprot_features_combined.tsv\")\n",
    "\n",
    "os.system(\n",
    "    f\"python {filter_uniprot_hits} -i {input_uniprot_features} -o {output_filtered_list}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47328a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A5A3E0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A816JW31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A0V1PKQ0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A4D9DGK8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A068XZE9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11092</th>\n",
       "      <td>A0A921QVQ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11093</th>\n",
       "      <td>A0A9J7N7R5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11094</th>\n",
       "      <td>A9P810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11095</th>\n",
       "      <td>D8LXR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11096</th>\n",
       "      <td>I7AL42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11097 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           protid\n",
       "0          A5A3E0\n",
       "1      A0A816JW31\n",
       "2      A0A0V1PKQ0\n",
       "3      A0A4D9DGK8\n",
       "4      A0A068XZE9\n",
       "...           ...\n",
       "11092  A0A921QVQ9\n",
       "11093  A0A9J7N7R5\n",
       "11094      A9P810\n",
       "11095      D8LXR4\n",
       "11096      I7AL42\n",
       "\n",
       "[11097 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Renames first column to protid\n",
    "features_filtered = pd.read_csv(output_filtered_list, sep=\"\\t\", names=[\"protid\"])\n",
    "\n",
    "# Save uniprot_features file\n",
    "features_filtered.to_csv(output_filtered_list, sep=\"\\t\", index=None)\n",
    "display(features_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "431adb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep paths\n",
    "output_filter_hits = os.path.join(input_path, \"uniprot_features_filtered.tsv\")\n",
    "\n",
    "# Apply filter\n",
    "uniprot_features_combined = pd.read_csv(\"../input/uniprot_features_combined.tsv\", sep=\"\\t\")\n",
    "uniprot_features_filtered = uniprot_features_combined.merge(\n",
    "    features_filtered, on=\"protid\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Save uniprot_features file\n",
    "uniprot_features_filtered.to_csv(output_filter_hits, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc39c53",
   "metadata": {},
   "source": [
    "## Reformat features file\n",
    "\n",
    "The features file from UniProt must be reformatted slightly to work well with ProteinCartography. We reformatted the file to fit with guidelines listed [here](https://github.com/Arcadia-Science/ProteinCartography#feature-file-main-columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd69b425-8b2c-45d0-a5f6-2b44c2cc34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw features file\n",
    "\n",
    "uniprot_features_filtered = pd.read_csv(output_filter_hits, sep=\"\\t\")\n",
    "\n",
    "# Reformat lineage column\n",
    "\n",
    "lineage_string_splitter = lambda lineage_string: [\n",
    "    rank.split(\" (\")[0] for rank in lineage_string.split(\", \")\n",
    "]\n",
    "\n",
    "uniprot_features_filtered[\"Lineage\"] = uniprot_features_filtered[\n",
    "    \"Taxonomic lineage\"\n",
    "].apply(lineage_string_splitter)\n",
    "\n",
    "# Saves updated uniprot_features file\n",
    "\n",
    "uniprot_features_filtered.to_csv(\"../output/uniprot_features.tsv\", sep=\"\\t\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('2023-actin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b7a315fe08ff0b168d16f2611f571320d2d7703d2c71b188b7ecb60dc0e34fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
